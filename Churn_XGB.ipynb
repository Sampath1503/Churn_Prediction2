{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GnGA6ln9VoAd",
        "outputId": "60471772-f030-444f-d786-e9863ab3c31f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-3039725488.py:21: FutureWarning: Downcasting behavior in `replace` is deprecated and will be removed in a future version. To retain the old behavior, explicitly call `result.infer_objects(copy=False)`. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
            "  df.replace(['Nan', 'nan', 'NaN', 'NAN'], np.nan, inplace=True)\n",
            "/usr/local/lib/python3.12/dist-packages/sklearn/model_selection/_search.py:1108: UserWarning: One or more of the test scores are non-finite: [nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan]\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Best Logistic Regression Params: {'C': 0.01, 'penalty': 'l1', 'solver': 'liblinear'}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/xgboost/training.py:199: UserWarning: [08:34:02] WARNING: /workspace/src/learner.cc:790: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  bst.update(dtrain, iteration=i, fobj=obj)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "          no       0.97      0.90      0.94       859\n",
            "         yes       0.59      0.83      0.69       141\n",
            "\n",
            "    accuracy                           0.89      1000\n",
            "   macro avg       0.78      0.87      0.81      1000\n",
            "weighted avg       0.92      0.89      0.90      1000\n",
            "\n",
            "[[777  82]\n",
            " [ 24 117]]\n",
            "‚úÖ Saved: num_imputer.pkl, cat_imputer.pkl, encoder.pkl, scaler.pkl, xgb_model.pkl\n"
          ]
        }
      ],
      "source": [
        "# -----------------------------\n",
        "# 1Ô∏è‚É£ Imports\n",
        "# -----------------------------\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split, GridSearchCV\n",
        "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
        "from sklearn.impute import SimpleImputer\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from xgboost import XGBClassifier\n",
        "from imblearn.combine import SMOTEENN\n",
        "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
        "import joblib\n",
        "\n",
        "# -----------------------------\n",
        "# 2Ô∏è‚É£ Load Dataset\n",
        "# -----------------------------\n",
        "df = pd.read_excel(\"P585 Churn.xlsx\")\n",
        "\n",
        "df.columns = df.columns.str.strip()\n",
        "df.replace(['Nan', 'nan', 'NaN', 'NAN'], np.nan, inplace=True)\n",
        "df.drop(columns=['Unnamed: 0'], errors='ignore', inplace=True)\n",
        "\n",
        "# -----------------------------\n",
        "# 3Ô∏è‚É£ Feature & Target\n",
        "# -----------------------------\n",
        "X = df.drop('churn', axis=1)\n",
        "y = df['churn']\n",
        "\n",
        "# -----------------------------\n",
        "# 4Ô∏è‚É£ Train-Test Split\n",
        "# -----------------------------\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.2, random_state=42, stratify=y\n",
        ")\n",
        "\n",
        "# -----------------------------\n",
        "# 5Ô∏è‚É£ Impute + Encode\n",
        "# -----------------------------\n",
        "categorical_cols = ['state', 'voice.plan', 'intl.plan']\n",
        "numeric_cols = X_train.select_dtypes(include=np.number).columns\n",
        "\n",
        "num_imputer = SimpleImputer(strategy='median')\n",
        "cat_imputer = SimpleImputer(strategy='most_frequent')\n",
        "\n",
        "X_train_num = num_imputer.fit_transform(X_train[numeric_cols])\n",
        "X_test_num = num_imputer.transform(X_test[numeric_cols])\n",
        "\n",
        "X_train_cat = cat_imputer.fit_transform(X_train[categorical_cols])\n",
        "X_test_cat = cat_imputer.transform(X_test[categorical_cols])\n",
        "\n",
        "ohe = OneHotEncoder(handle_unknown='ignore', sparse_output=False)\n",
        "X_train_enc = ohe.fit_transform(X_train_cat)\n",
        "X_test_enc = ohe.transform(X_test_cat)\n",
        "\n",
        "X_train_final = np.hstack([X_train_num, X_train_enc])\n",
        "X_test_final = np.hstack([X_test_num, X_test_enc])\n",
        "\n",
        "# -----------------------------\n",
        "# 6Ô∏è‚É£ Scaling\n",
        "# -----------------------------\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train_final)\n",
        "X_test_scaled = scaler.transform(X_test_final)\n",
        "\n",
        "# -----------------------------\n",
        "# 7Ô∏è‚É£ SMOTEENN\n",
        "# -----------------------------\n",
        "smote_enn = SMOTEENN(random_state=42)\n",
        "X_res, y_res = smote_enn.fit_resample(X_train_scaled, y_train)\n",
        "\n",
        "# -----------------------------\n",
        "# 8Ô∏è‚É£ Logistic Regression + GridSearch\n",
        "# -----------------------------\n",
        "param_grid_lr = {\n",
        "    'C': [0.01, 0.1, 1, 10],\n",
        "    'penalty': ['l1', 'l2'],\n",
        "    'solver': ['liblinear', 'saga']\n",
        "}\n",
        "\n",
        "lr = LogisticRegression(max_iter=1000, random_state=42)\n",
        "grid_lr = GridSearchCV(lr, param_grid_lr, cv=5, scoring='f1', n_jobs=-1)\n",
        "grid_lr.fit(X_res, y_res)\n",
        "best_lr = grid_lr.best_estimator_\n",
        "\n",
        "print(\"Best Logistic Regression Params:\", grid_lr.best_params_)\n",
        "\n",
        "# -----------------------------\n",
        "# 9Ô∏è‚É£ XGBoost Model\n",
        "# -----------------------------\n",
        "y_res_encoded = (y_res == 'yes').astype(int)\n",
        "y_test_encoded = (y_test == 'yes').astype(int)\n",
        "\n",
        "xgb = XGBClassifier(\n",
        "    use_label_encoder=False,\n",
        "    eval_metric='logloss',\n",
        "    random_state=42,\n",
        "    scale_pos_weight=y_res_encoded.value_counts()[0] / y_res_encoded.value_counts()[1]\n",
        ")\n",
        "xgb.fit(X_res, y_res_encoded)\n",
        "\n",
        "# -----------------------------\n",
        "# üîü Threshold Prediction Function\n",
        "# -----------------------------\n",
        "def predict_with_threshold(model, X, threshold=0.4):\n",
        "    probs = model.predict_proba(X)[:, 1]\n",
        "    pred = np.where(probs >= threshold, 'yes', 'no')\n",
        "    return pred\n",
        "\n",
        "# -----------------------------\n",
        "# 1Ô∏è‚É£1Ô∏è‚É£ Final Evaluation on XGBoost\n",
        "# -----------------------------\n",
        "y_pred = predict_with_threshold(xgb, X_test_scaled, threshold=0.4)\n",
        "print(classification_report(y_test, y_pred))\n",
        "print(confusion_matrix(y_test, y_pred))\n",
        "\n",
        "# -----------------------------\n",
        "# 1Ô∏è‚É£2Ô∏è‚É£ Save Artifacts for Deployment\n",
        "# -----------------------------\n",
        "joblib.dump(num_imputer, \"num_imputer.pkl\")\n",
        "joblib.dump(cat_imputer, \"cat_imputer.pkl\")\n",
        "joblib.dump(ohe, \"encoder.pkl\")\n",
        "joblib.dump(scaler, \"scaler.pkl\")\n",
        "joblib.dump(xgb, \"xgb_model.pkl\")\n",
        "\n",
        "print(\"‚úÖ Saved: num_imputer.pkl, cat_imputer.pkl, encoder.pkl, scaler.pkl, xgb_model.pkl\")\n"
      ]
    }
  ]
}